{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOKMVGZ1DcUdWNK22yCR4W8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MRTAd89mUE5-"},"outputs":[],"source":["from langchain.chains import RetrievalQA\n","from langchain_upstage import UpstageEmbeddings, ChatUpstage\n","from langchain.vectorstores.pinecone import Pinecone\n","\n","llm = ChatUpstage(model = 'solar-pro')\n","embeddings = UpstageEmbeddings(model = 'embedding-query')\n","load_vec_db = Pinecone.from_existing_index(\n","    index_name='upstage-tax2-1500',\n","    embedding=embeddings\n",")\n","\n","retriever = load_vec_db.as_retriever()"]},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","dictionary = ['사람을 나타내는 표현 -> 거주자']\n","\n","prompt = ChatPromptTemplate.from_template(f\"\"\"\n","    사용자의 질문을 보고, 사전을 참고해서 질문을 기존 단어를 대체하여 변경해주세요.\n","    만약 변경할 필요가 없다면, 변경하지 않아도 됩니다.\n","\n","    사전 : {dictionary}\n","    질문 : {{query}}\n","\"\"\")\n","\n","\n","dictionary_chain = prompt | llm | StrOutputParser()\n","dictionary_chain.invoke({\"query\" : '연봉 5000만원 직장인의 종합소득세는?'})"],"metadata":{"id":"29LD1PcvUj2Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["qa_retriever = RetrievalQA.from_llm(\n","    llm = llm,\n","    retriever = retriever\n",")"],"metadata":{"id":"N1mQxjmkXYZG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tax_chain = {'query' : dictionary_chain} | qa_retriever\n","tax_chain.invoke({'query' : '연봉 5000만원인 직장인의 종합소득세는?'})"],"metadata":{"id":"sRczuzrvXZ7h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 전체"],"metadata":{"id":"O0s872HOYPGQ"}},{"cell_type":"code","source":["from langchain.chains import RetrievalQA\n","from langchain_upstage import UpstageEmbeddings, ChatUpstage\n","from langchain.vectorstores.pinecone import Pinecone\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","def get_llm_response(user_input):\n","    llm = ChatUpstage(model = 'solar-pro')\n","    embeddings = UpstageEmbeddings(model = 'embedding-query')\n","    load_vec_db = Pinecone.from_existing_index(\n","        index_name='upstage-tax2-1500',\n","        embedding=embeddings\n","    )\n","\n","    retriever = load_vec_db.as_retriever()\n","\n","    dictionary = ['사람을 나타내는 표현 -> 거주자']\n","\n","    prompt = ChatPromptTemplate.from_template(f\"\"\"\n","        사용자의 질문을 보고, 사전을 참고해서 질문을 기존 단어를 대체하여 변경해주세요.\n","        만약 변경할 필요가 없다면, 변경하지 않아도 됩니다.\n","\n","        사전 : {dictionary}\n","        질문 : {{query}}\n","    \"\"\")\n","\n","    qa_retriever = RetrievalQA.from_llm(\n","    llm = llm,\n","    retriever = retriever)\n","\n","    dictionary_chain = prompt | llm | StrOutputParser()\n","    tax_chain = {'query' : dictionary_chain} | qa_retriever\n","    llm_response = tax_chain.invoke({'query' : user_input})['result']\n","    return llm_response"],"metadata":{"id":"tm1F6XEDYQCS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 스트리밋 (run.py)"],"metadata":{"id":"1P-P2VBeZ1Hx"}},{"cell_type":"code","source":["import streamlit as st\n","from langchain_upstage import ChatUpstage\n","from dotenv import load_dotenv\n","from llm import get_llm_response\n","\n","load_dotenv()\n","\n","llm = ChatUpstage(model='solar-pro')\n","\n","if \"messages\" not in st.session_state:\n","    st.session_state.messages = []\n","\n","st.title('무엇이든 물어보세요!')\n","\n","# 기존 메시지 표시\n","for message in st.session_state.messages:\n","    with st.chat_message(message[\"role\"]):\n","        st.write(message[\"content\"])\n","\n","if user_input := st.chat_input(\"채팅을 입력해주세요.\"):\n","    st.session_state.messages.append({\"role\" : \"user\", \"content\" : user_input})\n","    with st.chat_message('user'):\n","        st.write(user_input)\n","\n","    llm_result  = get_llm_response(user_input)\n","    st.session_state.messages.append({\"role\" : \"assistant\", \"content\" : llm_result})\n","    with st.chat_message('assistant'):\n","        st.write_stream(llm_result)"],"metadata":{"id":"U2Cu5jukZ2xs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### llm.py"],"metadata":{"id":"7s_I-ayzc1Mg"}},{"cell_type":"code","source":["from langchain_upstage import UpstageEmbeddings, ChatUpstage\n","from langchain.vectorstores.pinecone import Pinecone\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain_community.chat_message_histories import ChatMessageHistory\n","from langchain_core.chat_history import BaseChatMessageHistory\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.runnables.history import RunnableWithMessageHistory\n","from langchain_core.prompts import (\n","    FewShotChatMessagePromptTemplate,\n","    ChatPromptTemplate\n",")\n","from example import answer_examples\n","\n","### Statefully manage chat history ###\n","store = {}\n","\n","def get_session_history(session_id: str) -> BaseChatMessageHistory:\n","    if session_id not in store:\n","        store[session_id] = ChatMessageHistory()\n","    return store[session_id]\n","\n","def get_llm():\n","    llm = ChatUpstage(model = 'solar-pro')\n","    return llm\n","\n","def get_dictionary_chain():\n","    llm = get_llm()\n","    dictionary = ['사람을 나타내는 표현 -> 거주자']\n","\n","    prompt = ChatPromptTemplate.from_template(f\"\"\"\n","        사용자의 질문을 보고, 사전을 참고해서 질문을 기존 단어를 대체하여 변경해주세요.\n","        만약 변경할 필요가 없다면, 변경하지 않아도 됩니다.\n","\n","        사전 : {dictionary}\n","        질문 : {{query}}\n","    \"\"\")\n","    dictionary_chain = prompt | llm | StrOutputParser()\n","    return dictionary_chain\n","\n","def get_retriever():\n","    embeddings = UpstageEmbeddings(model = 'embedding-query')\n","    load_vec_db = Pinecone.from_existing_index(\n","        index_name='upstage-tax2-1500',\n","        embedding=embeddings\n","    )\n","\n","    retriever = load_vec_db.as_retriever()\n","    return retriever\n","\n","def get_history_chain():\n","    llm = get_llm()\n","    retriever = get_retriever()\n","    ### Contextualize question ###\n","    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n","    which might reference context in the chat history, formulate a standalone question \\\n","    which can be understood without the chat history. Do NOT answer the question, \\\n","    just reformulate it if needed and otherwise return it as is.\"\"\"\n","    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n","        [\n","            (\"system\", contextualize_q_system_prompt),\n","            MessagesPlaceholder(\"chat_history\"),\n","            (\"human\", \"{input}\"),\n","        ]\n","    )\n","    history_aware_retriever = create_history_aware_retriever(\n","        llm, retriever, contextualize_q_prompt\n","    )\n","    return history_aware_retriever\n","\n","def get_qa_chain():\n","    llm = get_llm()\n","    history_aware_retriever = get_history_chain()\n","    ### Answer question ###\n","    qa_system_prompt = \"\"\"\n","    당신은 소득세법 전문가입니다. 사용자의 소득세법에 관한 질문에 답변해주세요.\n","    답변은 아래 [조건]을 따릅니다.\n","\n","    [조건]\n","    - 아래 제공 된 내용(context)를 활용해서 답변해주세요.\n","    - 답변을 알 수 없다면 모른다고 답변해주세요.\n","    - 답변은 '소득세법 (XX조)에 따르면'이라고 시작해주세요.\n","    - 3문장 이내로 짧게 대답해주세요.\n","\n","    {context}\"\"\"\n","\n","    example_prompt = ChatPromptTemplate.from_messages(\n","    [('human', '{input}'), ('ai', '{answer}')]\n","    )\n","\n","    few_shot_prompt = FewShotChatMessagePromptTemplate(\n","        examples=answer_examples,\n","        # This is a prompt template used to format each individual example.\n","        example_prompt=example_prompt,\n","    )\n","\n","    qa_prompt = ChatPromptTemplate.from_messages(\n","        [\n","            (\"system\", qa_system_prompt),\n","            few_shot_prompt,\n","            MessagesPlaceholder(\"chat_history\"),\n","            (\"human\", \"{input}\"),\n","        ]\n","    )\n","    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n","\n","    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n","\n","    conversational_rag_chain = RunnableWithMessageHistory(\n","        rag_chain,\n","        get_session_history,\n","        input_messages_key=\"input\",\n","        history_messages_key=\"chat_history\",\n","        output_messages_key=\"answer\",\n","    ).pick('answer')\n","    return conversational_rag_chain\n","\n","def get_llm_response(user_input):\n","    dictionary_chain = get_dictionary_chain()\n","    qa_retriever = get_qa_chain()\n","\n","    tax_chain = {'input' : dictionary_chain} | qa_retriever\n","\n","    llm_response = tax_chain.stream(\n","        {\"query\": user_input},\n","    config={\n","        \"configurable\": {\"session_id\": \"abc123\"}\n","    },  # constructs a key \"abc123\" in `store`.\n","    )\n","\n","    return llm_response"],"metadata":{"id":"R7oZzpHtc2Z_"},"execution_count":null,"outputs":[]}]}