{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154400, 16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Preprocessed_Modeling.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['풍속(m/s)', '유의파고(m)'])\n",
    "y = df[['풍속(m/s)', '유의파고(m)']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 변수 중요도\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.font_manager as fm\n",
    "\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# importances = rf_model.feature_importances_\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# font_path = 'C:/Windows/Fonts/malgun.ttf'\n",
    "# font_prop = fm.FontProperties(fname=font_path, size=12)\n",
    "# plt.rcParams['font.family'] = font_prop.get_name()\n",
    "# plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# plt.title('변수 중요도', pad=10, fontweight='bold')\n",
    "# plt.bar(range(X.shape[1]), importances[indices], color=(52/255, 73/255, 94/255, 1.0), align='center')\n",
    "# tick_positions = np.arange(0, X.shape[1], 1)\n",
    "# plt.xticks(tick_positions, X.columns[indices], rotation=45)\n",
    "# plt.xlim([-1, X.shape[1]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 스케일링\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수 정의\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, pred):\n",
    "    train_r2 = model.score(X_train, y_train)  # R² 값\n",
    "    test_r2 = model.score(X_test, y_test)  # R² 값\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    \n",
    "    print(f\"훈련 R²: {train_r2:.4f}\")\n",
    "    print(f\"테스트 R²: {test_r2:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 0.9429\n",
      "테스트 R²: 0.9428\n",
      "MSE: 0.6363\n",
      "최적 하이퍼파라미터: {'alpha': 0.001, 'max_iter': 1000}\n",
      "최적 MSE: 0.6340\n",
      "훈련 R²: 0.9436\n",
      "테스트 R²: 0.9435\n",
      "MSE: 0.6333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "lasso_pred = lasso.predict(X_test_scaled)\n",
    "evaluate_model(lasso, X_train_scaled, X_test_scaled, y_train, y_test, lasso_pred)\n",
    "\n",
    "# 하이퍼파라미터 튜닝\n",
    "param_grid_lasso = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'max_iter': [1000, 10000, 100000]\n",
    "}\n",
    "lasso_search = GridSearchCV(Lasso(), param_grid_lasso, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "lasso_params = lasso_search.best_params_\n",
    "lasso_score = -lasso_search.best_score_  # MSE가 음수로 반환되므로 부호를 반전시킴\n",
    "print(f\"최적 하이퍼파라미터: {lasso_params}\")\n",
    "print(f\"최적 MSE: {lasso_score:.4f}\")\n",
    "\n",
    "# 최적 하이퍼파라미터 사용하여 재학습\n",
    "lasso_best = Lasso(**lasso_params)\n",
    "lasso_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "lasso_pred_best = lasso_best.predict(X_test_scaled)\n",
    "evaluate_model(lasso_best, X_train_scaled, X_test_scaled, y_train, y_test, lasso_pred_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 0.9436\n",
      "테스트 R²: 0.9435\n",
      "MSE: 0.6333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear = LinearRegression()\n",
    "linear.fit(X_train_scaled, y_train)\n",
    "linear_pred = linear.predict(X_test_scaled)\n",
    "evaluate_model(linear, X_train_scaled, X_test_scaled, y_train, y_test, linear_pred)\n",
    "\n",
    "# 하이퍼파라미터가 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 0.9436\n",
      "테스트 R²: 0.9435\n",
      "MSE: 0.6333\n",
      "최적 Ridge 하이퍼파라미터: {'alpha': 10}\n",
      "최적 Ridge MSE: 0.6339\n",
      "훈련 R²: 0.9436\n",
      "테스트 R²: 0.9435\n",
      "MSE: 0.6333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "ridge_pred = ridge.predict(X_test_scaled)\n",
    "evaluate_model(ridge, X_train_scaled, X_test_scaled, y_train, y_test, ridge_pred)\n",
    "\n",
    "# 하이퍼파라미터 튜닝\n",
    "ridge_param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "ridge_grid_search = GridSearchCV(Ridge(), ridge_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "ridge_best_params = ridge_grid_search.best_params_\n",
    "ridge_best_score = -ridge_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "print(f\"최적 Ridge 하이퍼파라미터: {ridge_best_params}\")\n",
    "print(f\"최적 Ridge MSE: {ridge_best_score:.4f}\")\n",
    "\n",
    "# 최적 하이퍼파라미터 사용하여 재학습\n",
    "ridge_best_model = Ridge(**ridge_best_params)\n",
    "ridge_best_model.fit(X_train_scaled, y_train)\n",
    "ridge_best_pred = ridge_best_model.predict(X_test_scaled)\n",
    "evaluate_model(ridge_best_model, X_train_scaled, X_test_scaled, y_train, y_test, ridge_best_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 0.7592\n",
      "테스트 R²: 0.7593\n",
      "MSE: 1.8136\n",
      "최적 ElasticNet 하이퍼파라미터: {'alpha': 0.001, 'l1_ratio': 0.1}\n",
      "최적 ElasticNet MSE: 0.6339\n",
      "훈련 R²: 0.9436\n",
      "테스트 R²: 0.9435\n",
      "MSE: 0.6333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5)  # 기본적으로 alpha=1.0, l1_ratio=0.5로 설정\n",
    "elastic_net.fit(X_train_scaled, y_train)\n",
    "elastic_net_pred = elastic_net.predict(X_test_scaled)\n",
    "evaluate_model(elastic_net, X_train_scaled, X_test_scaled, y_train, y_test, elastic_net_pred)\n",
    "\n",
    "# 하이퍼파라미터 튜닝\n",
    "param_grid_en = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]  # L1과 L2의 비율을 조정\n",
    "}\n",
    "elastic_net_grid_search = GridSearchCV(ElasticNet(), param_grid_en, cv=5, scoring='neg_mean_squared_error')\n",
    "elastic_net_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "elastic_net_best_params = elastic_net_grid_search.best_params_\n",
    "elastic_net_best_score = -elastic_net_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "print(f\"최적 ElasticNet 하이퍼파라미터: {elastic_net_best_params}\")\n",
    "print(f\"최적 ElasticNet MSE: {elastic_net_best_score:.4f}\")\n",
    "\n",
    "# 최적 하이퍼파라미터 사용하여 재학습\n",
    "elastic_net_best = ElasticNet(**elastic_net_best_params)\n",
    "elastic_net_best.fit(X_train_scaled, y_train)\n",
    "elastic_net_best_pred = elastic_net_best.predict(X_test_scaled)\n",
    "evaluate_model(elastic_net_best, X_train_scaled, X_test_scaled, y_train, y_test, elastic_net_best_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 1.0000\n",
      "테스트 R²: 0.9710\n",
      "MSE: 0.2995\n",
      "최적 DecisionTreeRegressor 하이퍼파라미터: {'max_depth': 10, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "최적 DecisionTreeRegressor MSE: 0.2578\n",
      "훈련 R²: 0.9738\n",
      "테스트 R²: 0.9707\n",
      "MSE: 0.2538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
    "dt_regressor.fit(X_train_scaled, y_train)\n",
    "dt_pred = dt_regressor.predict(X_test_scaled)\n",
    "evaluate_model(dt_regressor, X_train_scaled, X_test_scaled, y_train, y_test, dt_pred)\n",
    "\n",
    "# 하이퍼파라미터 튜닝\n",
    "param_grid_dt = {\n",
    "    'max_depth': [3, 5, 10, None],  # 트리의 최대 깊이\n",
    "    'min_samples_split': [2, 5, 10],  # 분할을 위한 최소 샘플 수\n",
    "    'min_samples_leaf': [1, 2, 4],  # 리프 노드의 최소 샘플 수\n",
    "    'max_features': [None, 'sqrt', 'log2'],  # 분할을 위한 최대 특성 수\n",
    "}\n",
    "\n",
    "dt_grid_search = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid_dt, cv=5, scoring='neg_mean_squared_error')\n",
    "dt_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "dt_best_params = dt_grid_search.best_params_\n",
    "dt_best_score = -dt_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "print(f\"최적 DecisionTreeRegressor 하이퍼파라미터: {dt_best_params}\")\n",
    "print(f\"최적 DecisionTreeRegressor MSE: {dt_best_score:.4f}\")\n",
    "\n",
    "# 최적 하이퍼파라미터 사용하여 재학습\n",
    "dt_best_model = DecisionTreeRegressor(**dt_best_params, random_state=42)\n",
    "dt_best_model.fit(X_train_scaled, y_train)\n",
    "dt_best_pred = dt_best_model.predict(X_test_scaled)\n",
    "evaluate_model(dt_best_model, X_train_scaled, X_test_scaled, y_train, y_test, dt_best_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# rf_regressor = RandomForestRegressor(random_state=42)\n",
    "# rf_regressor.fit(X_train_scaled, y_train)\n",
    "# rf_pred = rf_regressor.predict(X_test_scaled)\n",
    "# evaluate_model(rf_regressor, X_train_scaled, X_test_scaled, y_train, y_test, rf_pred)\n",
    "\n",
    "# # 하이퍼파라미터 튜닝\n",
    "# param_grid_rf = {\n",
    "#     'n_estimators': [50, 100, 200],  # 트리의 개수\n",
    "#     'max_depth': [None, 10, 20, 30],  # 트리의 최대 깊이\n",
    "#     'min_samples_split': [2, 5, 10],  # 분할을 위한 최소 샘플 수\n",
    "#     'min_samples_leaf': [1, 2, 4],  # 리프 노드의 최소 샘플 수\n",
    "#     'max_features': ['auto', 'sqrt', 'log2'],  # 분할을 위한 최대 특성 수\n",
    "#     'bootstrap': [True, False]  # 부트스트랩 샘플링 여부\n",
    "# }\n",
    "# rf_grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid_rf, cv=5, scoring='neg_mean_squared_error')\n",
    "# rf_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# rf_best_params = rf_grid_search.best_params_\n",
    "# rf_best_score = -rf_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "# print(f\"최적 RandomForestRegressor 하이퍼파라미터: {rf_best_params}\")\n",
    "# print(f\"최적 RandomForestRegressor MSE: {rf_best_score:.4f}\")\n",
    "\n",
    "# # 최적 하이퍼파라미터 사용하여 재학습\n",
    "# rf_best_model = RandomForestRegressor(**rf_best_params, random_state=42)\n",
    "# rf_best_model.fit(X_train_scaled, y_train)\n",
    "# rf_best_pred = rf_best_model.predict(X_test_scaled)\n",
    "# evaluate_model(rf_best_model, X_train_scaled, X_test_scaled, y_train, y_test, rf_best_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# y_train_GBR = y_train.values.ravel()  # pandas DataFrame을 numpy array로 변환하고 ravel() 호출\n",
    "\n",
    "# gb_regressor = GradientBoostingRegressor(random_state=42)\n",
    "# gb_regressor.fit(X_train_scaled, y_train_GBR)\n",
    "# gb_pred = gb_regressor.predict(X_test_scaled)\n",
    "# evaluate_model(gb_regressor, X_train_scaled, X_test_scaled, y_train_GBR, y_test, gb_pred)\n",
    "\n",
    "# # 하이퍼파라미터 튜닝\n",
    "# param_grid_gb = {\n",
    "#     'n_estimators': [50, 100, 200],  # 트리의 개수\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],  # 학습률\n",
    "#     'max_depth': [3, 5, 10],  # 트리의 최대 깊이\n",
    "#     'min_samples_split': [2, 5, 10],  # 분할을 위한 최소 샘플 수\n",
    "#     'min_samples_leaf': [1, 2, 4],  # 리프 노드의 최소 샘플 수\n",
    "#     'subsample': [0.8, 0.9, 1.0],  # 부트스트랩 샘플 비율\n",
    "# }\n",
    "# gb_grid_search = GridSearchCV(GradientBoostingRegressor(random_state=42), param_grid_gb, cv=5, scoring='neg_mean_squared_error')\n",
    "# gb_grid_search.fit(X_train_scaled, y_train_GBR)\n",
    "\n",
    "# gb_best_params = gb_grid_search.best_params_\n",
    "# gb_best_score = -gb_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "# print(f\"최적 GradientBoostingRegressor 하이퍼파라미터: {gb_best_params}\")\n",
    "# print(f\"최적 GradientBoostingRegressor MSE: {gb_best_score:.4f}\")\n",
    "\n",
    "# # 최적 하이퍼파라미터 사용하여 재학습\n",
    "# gb_best_model = GradientBoostingRegressor(**gb_best_params, random_state=42)\n",
    "# gb_best_model.fit(X_train_scaled, y_train_GBR)\n",
    "# gb_best_pred = gb_best_model.predict(X_test_scaled)\n",
    "# evaluate_model(gb_best_model, X_train_scaled, X_test_scaled, y_train_GBR, y_test, gb_best_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 0.9848\n",
      "테스트 R²: 0.9846\n",
      "MSE: 0.1713\n",
      "Fitting 3 folds for each of 2592 candidates, totalling 7776 fits\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=adam; total time=  51.7s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=adam; total time=  55.6s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=adam; total time=  41.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=lbfgs; total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=lbfgs; total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=lbfgs; total time= 2.9min\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=sgd; total time=  45.1s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=sgd; total time=  47.1s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=sgd; total time=  52.7s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=2000, solver=adam; total time=  44.3s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=2000, solver=adam; total time=  47.9s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=2000, solver=adam; total time=  32.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=2000, solver=lbfgs; total time= 5.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=2000, solver=lbfgs; total time= 5.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=2000, solver=lbfgs; total time= 5.5min\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=2000, solver=sgd; total time=  44.5s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=2000, solver=sgd; total time=  45.9s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=2000, solver=sgd; total time=  51.5s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=1000, solver=adam; total time=  43.9s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=1000, solver=adam; total time=  47.2s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=1000, solver=adam; total time=  32.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=1000, solver=lbfgs; total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=1000, solver=lbfgs; total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=1000, solver=lbfgs; total time= 2.9min\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=1000, solver=sgd; total time=   6.5s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=1000, solver=sgd; total time=   6.5s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=1000, solver=sgd; total time=   6.7s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=2000, solver=adam; total time=  45.1s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=2000, solver=adam; total time=  48.1s\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=2000, solver=adam; total time=  33.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=2000, solver=lbfgs; total time= 6.2min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 24\u001b[0m\n\u001b[0;32m     14\u001b[0m param_grid_mlp \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_layer_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m: [(\u001b[38;5;241m50\u001b[39m,), (\u001b[38;5;241m100\u001b[39m,), (\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m), (\u001b[38;5;241m200\u001b[39m,)],  \u001b[38;5;66;03m# 은닉층의 크기\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# 활성화 함수\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m]  \u001b[38;5;66;03m# 미니배치 크기\u001b[39;00m\n\u001b[0;32m     22\u001b[0m }\n\u001b[0;32m     23\u001b[0m mlp_grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(MLPRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m), param_grid_mlp, cv\u001b[38;5;241m=\u001b[39mcv, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m mlp_grid_search\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train)\n\u001b[0;32m     26\u001b[0m mlp_best_params \u001b[38;5;241m=\u001b[39m mlp_grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[0;32m     27\u001b[0m mlp_best_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmlp_grid_search\u001b[38;5;241m.\u001b[39mbest_score_  \u001b[38;5;66;03m# MSE는 음수로 반환되므로 부호를 반전시킴\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1572\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    965\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    966\u001b[0m         clone(base_estimator),\n\u001b[0;32m    967\u001b[0m         X,\n\u001b[0;32m    968\u001b[0m         y,\n\u001b[0;32m    969\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    970\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    971\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    972\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    973\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    975\u001b[0m     )\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    979\u001b[0m     )\n\u001b[0;32m    980\u001b[0m )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:751\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    735\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and target(s) y.\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \n\u001b[0;32m    737\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;124;03m        Returns a trained MLP model.\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, incremental\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:488\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# Run the LBFGS solver\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_lbfgs(\n\u001b[0;32m    489\u001b[0m         X, y, activations, deltas, coef_grads, intercept_grads, layer_units\n\u001b[0;32m    490\u001b[0m     )\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# validate parameter weights\u001b[39;00m\n\u001b[0;32m    493\u001b[0m weights \u001b[38;5;241m=\u001b[39m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoefs_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepts_)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:532\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit_lbfgs\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    530\u001b[0m     iprint \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 532\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39moptimize\u001b[38;5;241m.\u001b[39mminimize(\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_grad_lbfgs,\n\u001b[0;32m    534\u001b[0m     packed_coef_inter,\n\u001b[0;32m    535\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL-BFGS-B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    536\u001b[0m     jac\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    537\u001b[0m     options\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxfun\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_fun,\n\u001b[0;32m    539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m\"\u001b[39m: iprint,\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgtol\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[0;32m    542\u001b[0m     },\n\u001b[0;32m    543\u001b[0m     args\u001b[38;5;241m=\u001b[39m(X, y, activations, deltas, coef_grads, intercept_grads),\n\u001b[0;32m    544\u001b[0m )\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m _check_optimize_result(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m, opt_res, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter)\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_ \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:713\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    714\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    716\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    717\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:407\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    401\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:296\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl()\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:79\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_if_needed(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:73\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 73\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:281\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._loss_grad_lbfgs\u001b[1;34m(self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the MLP loss function and its corresponding derivatives\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03mwith respect to the different parameters given in the initialization.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;124;03mgrad : array-like, shape (number of nodes of all layers,)\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unpack(packed_coef_inter)\n\u001b[1;32m--> 281\u001b[0m loss, coef_grads, intercept_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backprop(\n\u001b[0;32m    282\u001b[0m     X, y, activations, deltas, coef_grads, intercept_grads\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    284\u001b[0m grad \u001b[38;5;241m=\u001b[39m _pack(coef_grads, intercept_grads)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, grad\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:357\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._backprop\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# Iterate over the hidden layers\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers_ \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 357\u001b[0m     deltas[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m safe_sparse_dot(deltas[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoefs_[i]\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    358\u001b[0m     inplace_derivative(activations[i], deltas[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss_grad(\n\u001b[0;32m    361\u001b[0m         i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, n_samples, activations, deltas, coef_grads, intercept_grads\n\u001b[0;32m    362\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:208\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m--> 208\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m ):\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:1513\u001b[0m, in \u001b[0;36missparse\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1508\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m sparray\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m _spbase\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[1;32m-> 1513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21missparse\u001b[39m(x):\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Is `x` of a sparse array or sparse matrix type?\u001b[39;00m\n\u001b[0;32m   1515\u001b[0m \n\u001b[0;32m   1516\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _spbase)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# KFold로 3개의 폴드로 설정\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "mlp_regressor = MLPRegressor(random_state=42, max_iter=500, solver='adam')\n",
    "mlp_regressor.fit(X_train_scaled, y_train)\n",
    "mlp_pred = mlp_regressor.predict(X_test_scaled)\n",
    "evaluate_model(mlp_regressor, X_train_scaled, X_test_scaled, y_train, y_test, mlp_pred)\n",
    "\n",
    "# 하이퍼파라미터 튜닝\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100, 100), (200,)],  # 은닉층의 크기\n",
    "    'activation': ['relu', 'tanh', 'logistic'],  # 활성화 함수\n",
    "    'solver': ['adam', 'lbfgs', 'sgd'],  # 최적화 알고리즘\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],  # L2 정규화의 강도\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],  # 학습률 일정\n",
    "    'max_iter': [1000, 2000],  # 최대 반복 횟수\n",
    "    'batch_size': ['auto', 32, 64]  # 미니배치 크기\n",
    "}\n",
    "mlp_grid_search = GridSearchCV(MLPRegressor(random_state=42), param_grid_mlp, cv=cv, scoring='neg_mean_squared_error', verbose=2)\n",
    "mlp_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "mlp_best_params = mlp_grid_search.best_params_\n",
    "mlp_best_score = -mlp_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "print(f\"최적 MLPRegressor 하이퍼파라미터: {mlp_best_params}\")\n",
    "print(f\"최적 MLPRegressor MSE: {mlp_best_score:.4f}\")\n",
    "\n",
    "# 최적 하이퍼파라미터 사용하여 재학습\n",
    "mlp_best_model = MLPRegressor(**mlp_best_params, random_state=42)\n",
    "mlp_best_model.fit(X_train_scaled, y_train)\n",
    "mlp_best_pred = mlp_best_model.predict(X_test_scaled)\n",
    "evaluate_model(mlp_best_model, X_train_scaled, X_test_scaled, y_train, y_test, mlp_best_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# '풍속(m/s)'와 '유의파고(m)'에 대해 각각 별도의 SVR 모델을 학습\n",
    "for target_column in y_train.columns:\n",
    "    target = y_train[target_column]\n",
    "    \n",
    "    # SVR 모델 생성\n",
    "    svr_regressor = SVR()\n",
    "    svr_regressor.fit(X_train_scaled, target)\n",
    "    svr_pred = svr_regressor.predict(X_test_scaled)\n",
    "    evaluate_model(svr_regressor, X_train_scaled, X_test_scaled, target, y_test[target_column], svr_pred)\n",
    "    \n",
    "    # 하이퍼파라미터 튜닝\n",
    "    param_grid_svr = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'epsilon': [0.01, 0.1, 0.5, 1],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'degree': [3, 4, 5],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "    \n",
    "    svr_grid_search = GridSearchCV(SVR(), param_grid_svr, cv=5, scoring='neg_mean_squared_error')\n",
    "    svr_grid_search.fit(X_train_scaled, target)\n",
    "\n",
    "    svr_best_params = svr_grid_search.best_params_\n",
    "    svr_best_score = -svr_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "    print(f\"최적 SVR 하이퍼파라미터 ({target_column}): {svr_best_params}\")\n",
    "    print(f\"최적 SVR MSE ({target_column}): {svr_best_score:.4f}\")\n",
    "\n",
    "    # 최적 하이퍼파라미터 사용하여 재학습\n",
    "    svr_best_model = SVR(**svr_best_params)\n",
    "    svr_best_model.fit(X_train_scaled, target)\n",
    "    svr_best_pred = svr_best_model.predict(X_test_scaled)\n",
    "    evaluate_model(svr_best_model, X_train_scaled, X_test_scaled, target, y_test[target_column], svr_best_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "knn_regressor = KNeighborsRegressor()\n",
    "knn_regressor.fit(X_train_scaled, y_train)\n",
    "knn_pred = knn_regressor.predict(X_test_scaled)\n",
    "evaluate_model(knn_regressor, X_train_scaled, X_test_scaled, y_train, y_test, knn_pred)\n",
    "\n",
    "# 하이퍼파라미터 튜닝\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],  # 이웃의 개수\n",
    "    'weights': ['uniform', 'distance'],  # 가중치 설정\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # 알고리즘 선택\n",
    "    'leaf_size': [20, 30, 40],  # ball_tree, kd_tree의 리프 크기\n",
    "    'p': [1, 2]  # 거리 계산 방식 (1=맨해튼, 2=유클리드)\n",
    "}\n",
    "knn_grid_search = GridSearchCV(KNeighborsRegressor(), param_grid_knn, cv=5, scoring='neg_mean_squared_error')\n",
    "knn_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "knn_best_params = knn_grid_search.best_params_\n",
    "knn_best_score = -knn_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "print(f\"최적 KNeighborsRegressor 하이퍼파라미터: {knn_best_params}\")\n",
    "print(f\"최적 KNeighborsRegressor MSE: {knn_best_score:.4f}\")\n",
    "\n",
    "# 최적 하이퍼파라미터 사용하여 재학습\n",
    "knn_best_model = KNeighborsRegressor(**knn_best_params)\n",
    "knn_best_model.fit(X_train_scaled, y_train)\n",
    "knn_best_pred = knn_best_model.predict(X_test_scaled)\n",
    "evaluate_model(knn_best_model, X_train_scaled, X_test_scaled, y_train, y_test, knn_best_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
