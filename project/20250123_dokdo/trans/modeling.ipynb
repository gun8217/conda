{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154400, 16)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Preprocessed_Modeling.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['풍속(m/s)', '유의파고(m)'])\n",
    "y = df[['풍속(m/s)', '유의파고(m)']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 변수 중요도\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.font_manager as fm\n",
    "\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# importances = rf_model.feature_importances_\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# font_path = 'C:/Windows/Fonts/malgun.ttf'\n",
    "# font_prop = fm.FontProperties(fname=font_path, size=12)\n",
    "# plt.rcParams['font.family'] = font_prop.get_name()\n",
    "# plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# plt.title('변수 중요도', pad=10, fontweight='bold')\n",
    "# plt.bar(range(X.shape[1]), importances[indices], color=(52/255, 73/255, 94/255, 1.0), align='center')\n",
    "# tick_positions = np.arange(0, X.shape[1], 1)\n",
    "# plt.xticks(tick_positions, X.columns[indices], rotation=45)\n",
    "# plt.xlim([-1, X.shape[1]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 스케일링\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수 정의\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, pred):\n",
    "    train_r2 = model.score(X_train, y_train)  # R² 값\n",
    "    test_r2 = model.score(X_test, y_test)  # R² 값\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    \n",
    "    print(f\"훈련 R²: {train_r2:.4f}\")\n",
    "    print(f\"테스트 R²: {test_r2:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 0.9429\n",
      "테스트 R²: 0.9428\n",
      "MSE: 0.6363\n",
      "최적 하이퍼파라미터: {'alpha': 0.001, 'max_iter': 1000}\n",
      "최적 MSE: 0.6340\n",
      "훈련 R²: 0.9436\n",
      "테스트 R²: 0.9435\n",
      "MSE: 0.6333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "lasso_pred = lasso.predict(X_test_scaled)\n",
    "evaluate_model(lasso, X_train_scaled, X_test_scaled, y_train, y_test, lasso_pred)\n",
    "\n",
    "# 하이퍼파라미터 튜닝\n",
    "param_grid_lasso = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'max_iter': [1000, 10000, 100000]\n",
    "}\n",
    "lasso_search = GridSearchCV(Lasso(), param_grid_lasso, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "lasso_params = lasso_search.best_params_\n",
    "lasso_score = -lasso_search.best_score_  # MSE가 음수로 반환되므로 부호를 반전시킴\n",
    "print(f\"최적 하이퍼파라미터: {lasso_params}\")\n",
    "print(f\"최적 MSE: {lasso_score:.4f}\")\n",
    "\n",
    "# 최적 하이퍼파라미터 사용하여 재학습\n",
    "lasso_best = Lasso(**lasso_params)\n",
    "lasso_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "lasso_pred_best = lasso_best.predict(X_test_scaled)\n",
    "evaluate_model(lasso_best, X_train_scaled, X_test_scaled, y_train, y_test, lasso_pred_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 0.9436\n",
      "테스트 R²: 0.9435\n",
      "MSE: 0.6333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear = LinearRegression()\n",
    "linear.fit(X_train_scaled, y_train)\n",
    "linear_pred = linear.predict(X_test_scaled)\n",
    "evaluate_model(linear, X_train_scaled, X_test_scaled, y_train, y_test, linear_pred)\n",
    "\n",
    "# 하이퍼파라미터가 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 0.9436\n",
      "테스트 R²: 0.9435\n",
      "MSE: 0.6333\n",
      "최적 Ridge 하이퍼파라미터: {'alpha': 10}\n",
      "최적 Ridge MSE: 0.6339\n",
      "훈련 R²: 0.9436\n",
      "테스트 R²: 0.9435\n",
      "MSE: 0.6333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "ridge_pred = ridge.predict(X_test_scaled)\n",
    "evaluate_model(ridge, X_train_scaled, X_test_scaled, y_train, y_test, ridge_pred)\n",
    "\n",
    "# 하이퍼파라미터 튜닝\n",
    "ridge_param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "ridge_grid_search = GridSearchCV(Ridge(), ridge_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "ridge_best_params = ridge_grid_search.best_params_\n",
    "ridge_best_score = -ridge_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "print(f\"최적 Ridge 하이퍼파라미터: {ridge_best_params}\")\n",
    "print(f\"최적 Ridge MSE: {ridge_best_score:.4f}\")\n",
    "\n",
    "# 최적 하이퍼파라미터 사용하여 재학습\n",
    "ridge_best_model = Ridge(**ridge_best_params)\n",
    "ridge_best_model.fit(X_train_scaled, y_train)\n",
    "ridge_best_pred = ridge_best_model.predict(X_test_scaled)\n",
    "evaluate_model(ridge_best_model, X_train_scaled, X_test_scaled, y_train, y_test, ridge_best_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 0.7592\n",
      "테스트 R²: 0.7593\n",
      "MSE: 1.8136\n",
      "최적 ElasticNet 하이퍼파라미터: {'alpha': 0.001, 'l1_ratio': 0.1}\n",
      "최적 ElasticNet MSE: 0.6339\n",
      "훈련 R²: 0.9436\n",
      "테스트 R²: 0.9435\n",
      "MSE: 0.6333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5)  # 기본적으로 alpha=1.0, l1_ratio=0.5로 설정\n",
    "elastic_net.fit(X_train_scaled, y_train)\n",
    "elastic_net_pred = elastic_net.predict(X_test_scaled)\n",
    "evaluate_model(elastic_net, X_train_scaled, X_test_scaled, y_train, y_test, elastic_net_pred)\n",
    "\n",
    "# 하이퍼파라미터 튜닝\n",
    "param_grid_en = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]  # L1과 L2의 비율을 조정\n",
    "}\n",
    "elastic_net_grid_search = GridSearchCV(ElasticNet(), param_grid_en, cv=5, scoring='neg_mean_squared_error')\n",
    "elastic_net_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "elastic_net_best_params = elastic_net_grid_search.best_params_\n",
    "elastic_net_best_score = -elastic_net_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "print(f\"최적 ElasticNet 하이퍼파라미터: {elastic_net_best_params}\")\n",
    "print(f\"최적 ElasticNet MSE: {elastic_net_best_score:.4f}\")\n",
    "\n",
    "# 최적 하이퍼파라미터 사용하여 재학습\n",
    "elastic_net_best = ElasticNet(**elastic_net_best_params)\n",
    "elastic_net_best.fit(X_train_scaled, y_train)\n",
    "elastic_net_best_pred = elastic_net_best.predict(X_test_scaled)\n",
    "evaluate_model(elastic_net_best, X_train_scaled, X_test_scaled, y_train, y_test, elastic_net_best_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 1.0000\n",
      "테스트 R²: 0.9710\n",
      "MSE: 0.2995\n",
      "최적 DecisionTreeRegressor 하이퍼파라미터: {'max_depth': 10, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "최적 DecisionTreeRegressor MSE: 0.2578\n",
      "훈련 R²: 0.9738\n",
      "테스트 R²: 0.9707\n",
      "MSE: 0.2538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
    "dt_regressor.fit(X_train_scaled, y_train)\n",
    "dt_pred = dt_regressor.predict(X_test_scaled)\n",
    "evaluate_model(dt_regressor, X_train_scaled, X_test_scaled, y_train, y_test, dt_pred)\n",
    "\n",
    "# 하이퍼파라미터 튜닝\n",
    "param_grid_dt = {\n",
    "    'max_depth': [3, 5, 10, None],  # 트리의 최대 깊이\n",
    "    'min_samples_split': [2, 5, 10],  # 분할을 위한 최소 샘플 수\n",
    "    'min_samples_leaf': [1, 2, 4],  # 리프 노드의 최소 샘플 수\n",
    "    'max_features': [None, 'sqrt', 'log2'],  # 분할을 위한 최대 특성 수\n",
    "}\n",
    "\n",
    "dt_grid_search = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid_dt, cv=5, scoring='neg_mean_squared_error')\n",
    "dt_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "dt_best_params = dt_grid_search.best_params_\n",
    "dt_best_score = -dt_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "print(f\"최적 DecisionTreeRegressor 하이퍼파라미터: {dt_best_params}\")\n",
    "print(f\"최적 DecisionTreeRegressor MSE: {dt_best_score:.4f}\")\n",
    "\n",
    "# 최적 하이퍼파라미터 사용하여 재학습\n",
    "dt_best_model = DecisionTreeRegressor(**dt_best_params, random_state=42)\n",
    "dt_best_model.fit(X_train_scaled, y_train)\n",
    "dt_best_pred = dt_best_model.predict(X_test_scaled)\n",
    "evaluate_model(dt_best_model, X_train_scaled, X_test_scaled, y_train, y_test, dt_best_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 0.9982\n",
      "테스트 R²: 0.9876\n",
      "MSE: 0.1375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "rf_regressor.fit(X_train_scaled, y_train)\n",
    "rf_pred = rf_regressor.predict(X_test_scaled)\n",
    "evaluate_model(rf_regressor, X_train_scaled, X_test_scaled, y_train, y_test, rf_pred)\n",
    "\n",
    "# 하이퍼파라미터 튜닝\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],  # 트리의 개수\n",
    "    'max_depth': [None, 10, 20, 30],  # 트리의 최대 깊이\n",
    "    'min_samples_split': [2, 5, 10],  # 분할을 위한 최소 샘플 수\n",
    "    'min_samples_leaf': [1, 2, 4],  # 리프 노드의 최소 샘플 수\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],  # 분할을 위한 최대 특성 수\n",
    "    'bootstrap': [True, False]  # 부트스트랩 샘플링 여부\n",
    "}\n",
    "rf_grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid_rf, cv=5, scoring='neg_mean_squared_error')\n",
    "rf_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "rf_best_params = rf_grid_search.best_params_\n",
    "rf_best_score = -rf_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "print(f\"최적 RandomForestRegressor 하이퍼파라미터: {rf_best_params}\")\n",
    "print(f\"최적 RandomForestRegressor MSE: {rf_best_score:.4f}\")\n",
    "\n",
    "# 최적 하이퍼파라미터 사용하여 재학습\n",
    "rf_best_model = RandomForestRegressor(**rf_best_params, random_state=42)\n",
    "rf_best_model.fit(X_train_scaled, y_train)\n",
    "rf_best_pred = rf_best_model.predict(X_test_scaled)\n",
    "evaluate_model(rf_best_model, X_train_scaled, X_test_scaled, y_train, y_test, rf_best_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# y_train_GBR = y_train.values.ravel()  # pandas DataFrame을 numpy array로 변환하고 ravel() 호출\n",
    "\n",
    "# gb_regressor = GradientBoostingRegressor(random_state=42)\n",
    "# gb_regressor.fit(X_train_scaled, y_train_GBR)\n",
    "# gb_pred = gb_regressor.predict(X_test_scaled)\n",
    "# evaluate_model(gb_regressor, X_train_scaled, X_test_scaled, y_train_GBR, y_test, gb_pred)\n",
    "\n",
    "# # 하이퍼파라미터 튜닝\n",
    "# param_grid_gb = {\n",
    "#     'n_estimators': [50, 100, 200],  # 트리의 개수\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],  # 학습률\n",
    "#     'max_depth': [3, 5, 10],  # 트리의 최대 깊이\n",
    "#     'min_samples_split': [2, 5, 10],  # 분할을 위한 최소 샘플 수\n",
    "#     'min_samples_leaf': [1, 2, 4],  # 리프 노드의 최소 샘플 수\n",
    "#     'subsample': [0.8, 0.9, 1.0],  # 부트스트랩 샘플 비율\n",
    "# }\n",
    "# gb_grid_search = GridSearchCV(GradientBoostingRegressor(random_state=42), param_grid_gb, cv=5, scoring='neg_mean_squared_error')\n",
    "# gb_grid_search.fit(X_train_scaled, y_train_GBR)\n",
    "\n",
    "# gb_best_params = gb_grid_search.best_params_\n",
    "# gb_best_score = -gb_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "# print(f\"최적 GradientBoostingRegressor 하이퍼파라미터: {gb_best_params}\")\n",
    "# print(f\"최적 GradientBoostingRegressor MSE: {gb_best_score:.4f}\")\n",
    "\n",
    "# # 최적 하이퍼파라미터 사용하여 재학습\n",
    "# gb_best_model = GradientBoostingRegressor(**gb_best_params, random_state=42)\n",
    "# gb_best_model.fit(X_train_scaled, y_train_GBR)\n",
    "# gb_best_pred = gb_best_model.predict(X_test_scaled)\n",
    "# evaluate_model(gb_best_model, X_train_scaled, X_test_scaled, y_train_GBR, y_test, gb_best_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 0.9819\n",
      "테스트 R²: 0.9816\n",
      "MSE: 0.2071\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# GradientBoostingRegressor를 다중 출력 회귀로 확장\n",
    "gb_regressor = GradientBoostingRegressor(random_state=42)\n",
    "multi_output_gb_regressor = MultiOutputRegressor(gb_regressor)\n",
    "\n",
    "multi_output_gb_regressor.fit(X_train_scaled, y_train)\n",
    "gb_pred = multi_output_gb_regressor.predict(X_test_scaled)\n",
    "evaluate_model(multi_output_gb_regressor, X_train_scaled, X_test_scaled, y_train, y_test, gb_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 0.9848\n",
      "테스트 R²: 0.9846\n",
      "MSE: 0.1713\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# KFold로 3개의 폴드로 설정\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "mlp_regressor = MLPRegressor(random_state=42, max_iter=500, solver='adam')\n",
    "mlp_regressor.fit(X_train_scaled, y_train)\n",
    "mlp_pred = mlp_regressor.predict(X_test_scaled)\n",
    "evaluate_model(mlp_regressor, X_train_scaled, X_test_scaled, y_train, y_test, mlp_pred)\n",
    "\n",
    "# # 하이퍼파라미터 튜닝\n",
    "# param_grid_mlp = {\n",
    "#     'hidden_layer_sizes': [(50,), (100,), (100, 100), (200,)],  # 은닉층의 크기\n",
    "#     'activation': ['relu', 'tanh', 'logistic'],  # 활성화 함수\n",
    "#     'solver': ['adam', 'lbfgs', 'sgd'],  # 최적화 알고리즘\n",
    "#     'alpha': [0.0001, 0.001, 0.01, 0.1],  # L2 정규화의 강도\n",
    "#     'learning_rate': ['constant', 'invscaling', 'adaptive'],  # 학습률 일정\n",
    "#     'max_iter': [1000, 2000],  # 최대 반복 횟수\n",
    "#     'batch_size': ['auto', 32, 64]  # 미니배치 크기\n",
    "# }\n",
    "# mlp_grid_search = GridSearchCV(MLPRegressor(random_state=42), param_grid_mlp, cv=cv, scoring='neg_mean_squared_error', verbose=2)\n",
    "# mlp_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# mlp_best_params = mlp_grid_search.best_params_\n",
    "# mlp_best_score = -mlp_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "# print(f\"최적 MLPRegressor 하이퍼파라미터: {mlp_best_params}\")\n",
    "# print(f\"최적 MLPRegressor MSE: {mlp_best_score:.4f}\")\n",
    "\n",
    "# # 최적 하이퍼파라미터 사용하여 재학습\n",
    "# mlp_best_model = MLPRegressor(**mlp_best_params, random_state=42)\n",
    "# mlp_best_model.fit(X_train_scaled, y_train)\n",
    "# mlp_best_pred = mlp_best_model.predict(X_test_scaled)\n",
    "# evaluate_model(mlp_best_model, X_train_scaled, X_test_scaled, y_train, y_test, mlp_best_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 0.9724\n",
      "테스트 R²: 0.9732\n",
      "MSE: 0.3577\n",
      "훈련 R²: 0.9960\n",
      "테스트 R²: 0.9959\n",
      "MSE: 0.0080\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# '풍속(m/s)'와 '유의파고(m)'에 대해 각각 별도의 SVR 모델을 학습\n",
    "for target_column in y_train.columns:\n",
    "    target = y_train[target_column]\n",
    "    \n",
    "    # SVR 모델 생성\n",
    "    svr_regressor = SVR()\n",
    "    svr_regressor.fit(X_train_scaled, target)\n",
    "    svr_pred = svr_regressor.predict(X_test_scaled)\n",
    "    evaluate_model(svr_regressor, X_train_scaled, X_test_scaled, target, y_test[target_column], svr_pred)\n",
    "    \n",
    "    # # 하이퍼파라미터 튜닝\n",
    "    # param_grid_svr = {\n",
    "    #     'C': [0.1, 1, 10, 100],\n",
    "    #     'epsilon': [0.01, 0.1, 0.5, 1],\n",
    "    #     'kernel': ['linear', 'rbf', 'poly'],\n",
    "    #     'degree': [3, 4, 5],\n",
    "    #     'gamma': ['scale', 'auto']\n",
    "    # }\n",
    "    \n",
    "    # svr_grid_search = GridSearchCV(SVR(), param_grid_svr, cv=5, scoring='neg_mean_squared_error')\n",
    "    # svr_grid_search.fit(X_train_scaled, target)\n",
    "\n",
    "    # svr_best_params = svr_grid_search.best_params_\n",
    "    # svr_best_score = -svr_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "    # print(f\"최적 SVR 하이퍼파라미터 ({target_column}): {svr_best_params}\")\n",
    "    # print(f\"최적 SVR MSE ({target_column}): {svr_best_score:.4f}\")\n",
    "\n",
    "    # # 최적 하이퍼파라미터 사용하여 재학습\n",
    "    # svr_best_model = SVR(**svr_best_params)\n",
    "    # svr_best_model.fit(X_train_scaled, target)\n",
    "    # svr_best_pred = svr_best_model.predict(X_test_scaled)\n",
    "    # evaluate_model(svr_best_model, X_train_scaled, X_test_scaled, target, y_test[target_column], svr_best_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 R²: 0.9843\n",
      "테스트 R²: 0.9759\n",
      "MSE: 0.2569\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "knn_regressor = KNeighborsRegressor()\n",
    "knn_regressor.fit(X_train_scaled, y_train)\n",
    "knn_pred = knn_regressor.predict(X_test_scaled)\n",
    "evaluate_model(knn_regressor, X_train_scaled, X_test_scaled, y_train, y_test, knn_pred)\n",
    "\n",
    "# # 하이퍼파라미터 튜닝\n",
    "# param_grid_knn = {\n",
    "#     'n_neighbors': [3, 5, 7, 9, 11],  # 이웃의 개수\n",
    "#     'weights': ['uniform', 'distance'],  # 가중치 설정\n",
    "#     'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # 알고리즘 선택\n",
    "#     'leaf_size': [20, 30, 40],  # ball_tree, kd_tree의 리프 크기\n",
    "#     'p': [1, 2]  # 거리 계산 방식 (1=맨해튼, 2=유클리드)\n",
    "# }\n",
    "# knn_grid_search = GridSearchCV(KNeighborsRegressor(), param_grid_knn, cv=5, scoring='neg_mean_squared_error')\n",
    "# knn_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# knn_best_params = knn_grid_search.best_params_\n",
    "# knn_best_score = -knn_grid_search.best_score_  # MSE는 음수로 반환되므로 부호를 반전시킴\n",
    "# print(f\"최적 KNeighborsRegressor 하이퍼파라미터: {knn_best_params}\")\n",
    "# print(f\"최적 KNeighborsRegressor MSE: {knn_best_score:.4f}\")\n",
    "\n",
    "# # 최적 하이퍼파라미터 사용하여 재학습\n",
    "# knn_best_model = KNeighborsRegressor(**knn_best_params)\n",
    "# knn_best_model.fit(X_train_scaled, y_train)\n",
    "# knn_best_pred = knn_best_model.predict(X_test_scaled)\n",
    "# evaluate_model(knn_best_model, X_train_scaled, X_test_scaled, y_train, y_test, knn_best_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
